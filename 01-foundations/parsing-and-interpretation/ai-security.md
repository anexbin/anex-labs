# AI Security & Red Teaming Relevance

## Prompt Injection = Parsing Failure
AI systems parse natural language input into internal instructions.
When untrusted input is treated as trusted intent, boundaries collapse.

## Boundary Breakdown
- User input overrides system intent
- Data becomes instructions
- Context becomes control

## Key Parallel
AI prompt parsing failures mirror classic interpreter vulnerabilities in software systems.

## Red Team Insight
Attacking how meaning is constructed scales better than attacking models directly.
